{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from math import *\n",
    "\n",
    "## 결측 위한 library - MICE\n",
    "from fancyimpute import IterativeImputer\n",
    "\n",
    "# scalar\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "## Regression / Classifier 모델\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    seed=37\n",
    "def seed_(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed=seed)\n",
    "seed_(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIMESTAMP DROP\n",
    "train = train.drop(['TIMESTAMP'], axis=1)\n",
    "test = test.drop(['TIMESTAMP'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINE Split Done.\n"
     ]
    }
   ],
   "source": [
    "# LINE별로 분리 해주세용\n",
    "train_T050304 = train[train['LINE']=='T050304'] # 78\n",
    "train_T050307 = train[train['LINE']=='T050307'] # 42\n",
    "train_T100304 = train[train['LINE']=='T100304'] # 175\n",
    "train_T100306 = train[train['LINE']=='T100306'] # 174\n",
    "train_T010306 = train[train['LINE']=='T010306'] # 70\n",
    "train_T010305 = train[train['LINE']=='T010305'] # 59\n",
    "\n",
    "test_T050304 = test[test['LINE']=='T050304']\n",
    "test_T050307 = test[test['LINE']=='T050307']\n",
    "test_T100304 = test[test['LINE']=='T100304']\n",
    "test_T100306 = test[test['LINE']=='T100306']\n",
    "test_T010306 = test[test['LINE']=='T010306']\n",
    "test_T010305 = test[test['LINE']=='T010305']\n",
    "\n",
    "print('LINE Split Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All NaN Check.\n",
      "All NaN Column Drop.\n"
     ]
    }
   ],
   "source": [
    "notwith1 = []\n",
    "notwith2 = []\n",
    "notwith3 = []\n",
    "notwith4 = []\n",
    "notwith5 = []\n",
    "notwith6 = []\n",
    "\n",
    "# product별 사용하지 않는 변수 확인\n",
    "for col in train_T050304.columns:\n",
    "    if pd.notnull(train_T050304[col]).sum()==0:\n",
    "        notwith1.append(col)\n",
    "\n",
    "for col in train_T050307.columns:\n",
    "    if pd.notnull(train_T050307[col]).sum()==0:\n",
    "        notwith2.append(col)\n",
    "\n",
    "for col in train_T100304.columns:\n",
    "    if pd.notnull(train_T100304[col]).sum()==0:\n",
    "        notwith3.append(col)\n",
    "        \n",
    "for col in train_T100306.columns:\n",
    "    if pd.notnull(train_T100306[col]).sum()==0:\n",
    "        notwith4.append(col)\n",
    "\n",
    "for col in train_T010306.columns:\n",
    "    if pd.notnull(train_T010306[col]).sum()==0:\n",
    "        notwith5.append(col)\n",
    "\n",
    "for col in train_T010305.columns:\n",
    "    if pd.notnull(train_T010305[col]).sum()==0:\n",
    "        notwith6.append(col)\n",
    "\n",
    "print('All NaN Check.')\n",
    "\n",
    "# 확인하고 제거\n",
    "for col in notwith1:\n",
    "    train_T050304 = train_T050304.drop([col], axis=1) # A_31 + T050304 , 1973 - 아래와는 1961 겹침 , 같은 A라인들과 765겹침\n",
    "    test_T050304 = test_T050304.drop([col], axis=1)\n",
    "\n",
    "for col in notwith2:\n",
    "    train_T050307 = train_T050307.drop([col], axis=1) # A_31 + T050307 , 1980\n",
    "    test_T050307 = test_T050307.drop([col], axis=1)\n",
    "\n",
    "for col in notwith3:\n",
    "    train_T100304 = train_T100304.drop([col], axis=1) # T(172) + O(3) + T100304 , 676 - 아래와는 670 겹침\n",
    "    test_T100304 = test_T100304.drop([col], axis=1)\n",
    "\n",
    "for col in notwith4:\n",
    "    train_T100306 = train_T100306.drop([col], axis=1) # T(171) + O(3) + T100306 , 676\n",
    "    test_T100306 = test_T100306.drop([col], axis=1)\n",
    "\n",
    "for col in notwith5:\n",
    "    train_T010306 = train_T010306.drop([col], axis=1) # A_31 + T010306 , 891 - 아래와는 883 겹침\n",
    "    test_T010306 = test_T010306.drop([col], axis=1)\n",
    "\n",
    "for col in notwith6:\n",
    "    train_T010305 = train_T010305.drop([col], axis=1) # A_31 + T010305 , 891\n",
    "    test_T010305 = test_T010305.drop([col], axis=1)\n",
    "\n",
    "print('All NaN Column Drop.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각자 제일 많이 겹치는 LINE 2개씩 묶는데 안겹치는 컬럼 찾고 \n",
    "list_train_T05 = list(set(train_T050304).difference(train_T050307))\n",
    "list_train_T10 = list(set(train_T100304).difference(train_T100306))\n",
    "list_train_T01 = list(set(train_T010306).difference(train_T010305))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그리고 일단 DROP!!!!! 이부분은 다시 확인해보자 유의한 컬럼 떨궜을수도 있음 ,,\n",
    "for col in list_train_T05:\n",
    "    try:\n",
    "        train_T050304 = train_T050304.drop([col], axis=1)\n",
    "        test_T050304 = test_T050304.drop([col], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        train_T050307 = train_T050307.drop([col], axis=1)\n",
    "        test_T050307 = test_T050307.drop([col], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for col in list_train_T10:\n",
    "    try:\n",
    "        train_T100304 = train_T100304.drop([col], axis=1)\n",
    "        test_T100304 = test_T100304.drop([col], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        train_T100306 = train_T100306.drop([col], axis=1)\n",
    "        test_T100306 = test_T100306.drop([col], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for col in list_train_T01:\n",
    "    try:\n",
    "        train_T010306 = train_T010306.drop([col], axis=1)\n",
    "        test_T010306 = test_T010306.drop([col], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        train_T010305 = train_T010305.drop([col], axis=1)\n",
    "        test_T010305 = test_T010305.drop([col], axis=1)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맞춘 친구들끼리 묶고\n",
    "train_T05 = pd.concat([train_T050304, train_T050307])\n",
    "train_T10 = pd.concat([train_T100304, train_T100306])\n",
    "train_T01 = pd.concat([train_T010306, train_T010305])\n",
    "\n",
    "test_T05 = pd.concat([test_T050304, test_T050307])\n",
    "test_T10 = pd.concat([test_T100304, test_T100306])\n",
    "test_T01 = pd.concat([test_T010306, test_T010305])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 분리했으니 사이좋게 레이블도 때고\n",
    "train_y1 = train_T05['Y_Class']\n",
    "train_y2 = train_T10['Y_Class']\n",
    "train_y3 = train_T01['Y_Class']\n",
    "\n",
    "train_T05 = train_T05.drop(['Y_Class'], axis=1)\n",
    "train_T10 = train_T10.drop(['Y_Class'], axis=1)\n",
    "train_T01 = train_T01.drop(['Y_Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE encode Done.\n",
      "LINE encod Done.\n"
     ]
    }
   ],
   "source": [
    "## 레이블인코더로 인코딩\n",
    "le1 = LabelEncoder()\n",
    "for x in [train_T01, train_T05, train_T10]:\n",
    "    le1 = le1.fit(x['PRODUCT_CODE'])\n",
    "    x['PRODUCT_CODE'] = le1.transform(x['PRODUCT_CODE'])\n",
    "        \n",
    "for t in [test_T01, test_T05, test_T10]:\n",
    "    for label in np.unique(t['PRODUCT_CODE']): \n",
    "        if label not in le1.classes_: \n",
    "            le1.classes_ = np.append(le1.classes_, label)\n",
    "    t['PRODUCT_CODE'] = le1.transform(t['PRODUCT_CODE'])\n",
    "print('CODE encode Done.')\n",
    "\n",
    "le2 = LabelEncoder()\n",
    "for x in [train_T01, train_T05, train_T10]:\n",
    "    le2 = le2.fit(x['LINE'])\n",
    "    x['LINE'] = le2.transform(x['LINE'])\n",
    "        \n",
    "for t in [test_T01, test_T05, test_T10]:\n",
    "    for label in np.unique(t['LINE']): \n",
    "        if label not in le2.classes_: \n",
    "            le2.classes_ = np.append(le2.classes_, label)\n",
    "    t['LINE'] = le2.transform(t['LINE'])\n",
    "print('LINE encod Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop PRODUCT_ID\n"
     ]
    }
   ],
   "source": [
    "## 프로덕트 각 key인 이름 제거\n",
    "train_T05 = train_T05.drop(['PRODUCT_ID'], axis=1)\n",
    "train_T10 = train_T10.drop(['PRODUCT_ID'], axis=1)\n",
    "train_T01 = train_T01.drop(['PRODUCT_ID'], axis=1)\n",
    "print('Drop PRODUCT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Fillna. (T10)\n",
      "Check NaN list. (T10)\n"
     ]
    }
   ],
   "source": [
    "# train_T10 : 결측 대치 하고 , 그래도 남은 애들 세보기\n",
    "coldict_T10 = {}\n",
    "nalist= []\n",
    "\n",
    "for col in train_T10.columns:\n",
    "    if pd.isna(train_T10[col]).sum() <= len(train_T10) * 0.1:\n",
    "        train_T10[col] = train_T10[col].fillna(np.nanmedian(train_T10[col])) # row*0.1 개 이하면 중앙값으로 결측 대치\n",
    "        try:\n",
    "            test_T10[col] = test_T10[col].fillna(np.nanmedian(train_T10[col]))\n",
    "        except:\n",
    "            pass\n",
    "print('Done Fillna. (T10)')\n",
    "\n",
    "for col in train_T10.columns:\n",
    "    if pd.isna(train_T10[col]).sum() > 0:\n",
    "        nalist.append(col)\n",
    "print('Check NaN list. (T10)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Fillna. (T01)\n",
      "Check NaN list. (T01)\n"
     ]
    }
   ],
   "source": [
    "# train_T01 : 결측 대치 하고 , 그래도 남은 애들 세보기\n",
    "coldict_T01 = {}\n",
    "nalist2 = []\n",
    "\n",
    "for col in train_T01.columns:\n",
    "    if pd.isna(train_T01[col]).sum() <= len(train_T01)*0.1:\n",
    "        train_T01[col] = train_T01[col].fillna(np.nanmedian(train_T01[col])) # row*0.1 개 이하면 중앙값으로 결측 대치\n",
    "        try:\n",
    "            test_T01[col] = test_T01[col].fillna(np.nanmedian(train_T01[col]))\n",
    "        except:\n",
    "            pass\n",
    "print('Done Fillna. (T01)')\n",
    "\n",
    "for col in train_T01.columns:\n",
    "    if pd.isna(train_T01[col]).sum() > 0:\n",
    "        nalist2.append(col)\n",
    "print('Check NaN list. (T01)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fillna. (T05)\n",
      "Check NaN list. (T05)\n"
     ]
    }
   ],
   "source": [
    "# train_T05 : 결측 대치 하고 , 그래도 남은 애들 세보기\n",
    "coldict_T05 = {}\n",
    "nalist3 = []\n",
    "\n",
    "for col in train_T05.columns:\n",
    "    if pd.isna(train_T05[col]).sum() <= len(train_T05)*0.1:\n",
    "        train_T05[col] = train_T05[col].fillna(np.nanmedian(train_T05[col])) # row*0.1 개 이하면 중앙값으로 결측 대치\n",
    "        try:\n",
    "            test_T05[col] = test_T05[col].fillna(np.nanmedian(train_T05[col]))\n",
    "        except:\n",
    "            pass\n",
    "print('Done fillna. (T05)')\n",
    "\n",
    "for col in train_T05.columns:\n",
    "    if pd.isna(train_T05[col]).sum() > 0:\n",
    "        nalist3.append(col)\n",
    "print('Check NaN list. (T05)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 결측값 만들려고 있는 애들끼리만 임시로 dataframe 만들기\n",
    "tmp_T05 = pd.concat([train_T05[nalist3]], axis=1)\n",
    "tmp_T01 = pd.concat([train_T01[nalist2]], axis=1)\n",
    "tmp_T10 = pd.concat([train_T10[nalist]], axis=1)\n",
    "\n",
    "tmp_test_T05 = pd.concat([test_T05[nalist3]], axis=1)\n",
    "tmp_test_T01 = pd.concat([test_T01[nalist2]], axis=1)\n",
    "tmp_test_T10 = pd.concat([test_T10[nalist]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer Done.\n",
      "Imputer Done.\n",
      "Imputer Done.\n"
     ]
    }
   ],
   "source": [
    "# 이걸로 결측대치\n",
    "imputer1 = IterativeImputer(random_state=37)\n",
    "imputer2 = IterativeImputer(random_state=37)\n",
    "imputer3 = IterativeImputer(random_state=37, max_iter=5)\n",
    "\n",
    "trans_x1 = imputer1.fit_transform(tmp_T05)\n",
    "trans_x2 = imputer2.fit_transform(tmp_T01)\n",
    "trans_x3 = imputer3.fit_transform(tmp_T10)\n",
    "\n",
    "test_trans_x1 = imputer1.transform(tmp_test_T05)\n",
    "print('Imputer Done.')\n",
    "test_trans_x2 = imputer2.transform(tmp_test_T01)\n",
    "print('Imputer Done.')\n",
    "test_trans_x3 = imputer3.transform(tmp_test_T10)\n",
    "print('Imputer Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 결측 대치한 후에 바까주기\n",
    "\n",
    "train_T05 = train_T05.drop(nalist3, axis=1)\n",
    "train_T01 = train_T01.drop(nalist2, axis=1)\n",
    "train_T10 = train_T10.drop(nalist, axis=1)\n",
    "\n",
    "test_T05 = test_T05.drop(nalist3, axis=1)\n",
    "test_T01 = test_T01.drop(nalist2, axis=1)\n",
    "test_T10 = test_T10.drop(nalist, axis=1)\n",
    "\n",
    "fill_T05 = pd.DataFrame(trans_x1, columns=tmp_T05.columns)\n",
    "fill_T01 = pd.DataFrame(trans_x2, columns=tmp_T01.columns)\n",
    "fill_T10 = pd.DataFrame(trans_x3, columns=tmp_T10.columns)\n",
    "\n",
    "test_fill_T05 = pd.DataFrame(test_trans_x1, columns=tmp_T05.columns)\n",
    "test_fill_T01 = pd.DataFrame(test_trans_x2, columns=tmp_T01.columns)\n",
    "test_fill_T10 = pd.DataFrame(test_trans_x3, columns=tmp_T10.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_T05 = pd.concat([train_T05], ignore_index=True)\n",
    "train_T01 = pd.concat([train_T01], ignore_index=True)\n",
    "train_T10 = pd.concat([train_T10], ignore_index=True)\n",
    "\n",
    "train_T05 = pd.concat([train_T05, fill_T05], axis=1)\n",
    "train_T01 = pd.concat([train_T01, fill_T01], axis=1)\n",
    "train_T10 = pd.concat([train_T10, fill_T10], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_T05 = pd.concat([test_T05], ignore_index=True)\n",
    "test_T01 = pd.concat([test_T01], ignore_index=True)\n",
    "test_T10 = pd.concat([test_T10], ignore_index=True)\n",
    "\n",
    "test_T05 = pd.concat([test_T05, test_fill_T05], axis=1)\n",
    "test_T01 = pd.concat([test_T01, test_fill_T01], axis=1)\n",
    "test_T10 = pd.concat([test_T10, test_fill_T10], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 한가지 값만 있는 데이터 떼기\n",
    "one_data_col_train_T05 = []\n",
    "one_data_col_train_T10 = []\n",
    "one_data_col_train_T01 = []\n",
    "\n",
    "for col in train_T05.filter(regex='X').columns:\n",
    "    if len(train_T05[col].unique().tolist()) ==1:\n",
    "        one_data_col_train_T05.append(col)\n",
    "\n",
    "for col in train_T10.filter(regex='X').columns:\n",
    "    if len(train_T10[col].unique().tolist()) ==1:\n",
    "        one_data_col_train_T10.append(col)\n",
    "\n",
    "for col in train_T01.filter(regex='X').columns:\n",
    "    if len(train_T01[col].unique().tolist()) ==1:\n",
    "        one_data_col_train_T01.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 합쳐보기\n",
    "train_tmp1 = pd.DataFrame()\n",
    "test_tmp1 = pd.DataFrame()\n",
    "\n",
    "train_tmp2 = pd.DataFrame()\n",
    "test_tmp2 = pd.DataFrame()\n",
    "\n",
    "train_tmp3 = pd.DataFrame()\n",
    "test_tmp3 = pd.DataFrame()\n",
    "\n",
    "for col in one_data_col_train_T05:\n",
    "    train_tmp1 = pd.concat([train_tmp1, train_T05[col]], axis=1)\n",
    "    test_tmp1 = pd.concat([test_tmp1, test_T05[col]], axis=1)\n",
    "    train_T05 = train_T05.drop([col], axis=1)\n",
    "    test_T05 = test_T05.drop([col], axis=1)\n",
    "\n",
    "for col in one_data_col_train_T10:\n",
    "    train_tmp2 = pd.concat([train_tmp2, train_T10[col]], axis=1)\n",
    "    test_tmp2 = pd.concat([test_tmp2, test_T10[col]], axis=1)\n",
    "    train_T10 = train_T10.drop([col], axis=1)\n",
    "    test_T10 = test_T10.drop([col], axis=1)\n",
    "\n",
    "for col in one_data_col_train_T01:\n",
    "    train_tmp3 = pd.concat([train_tmp3, train_T01[col]], axis=1)\n",
    "    test_tmp3 = pd.concat([test_tmp3, test_T01[col]], axis=1)\n",
    "    train_T01 = train_T01.drop([col], axis=1)\n",
    "    test_T01 = test_T01.drop([col], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 합친 column 지우는 columns들과 치환\n",
    "train_tmp_ = pd.concat([train_tmp1[train_tmp1.columns[0]]], axis=1)\n",
    "test_tmp_ = pd.concat([test_tmp1[test_tmp1.columns[0]]], axis=1)\n",
    "\n",
    "for col in train_tmp1.columns[1:]:\n",
    "    train_tmp_[train_tmp_.columns[0]] = train_tmp_[train_tmp_.columns[0]] + train_tmp1[col]\n",
    "for col in test_tmp1.columns[1:]:\n",
    "    test_tmp_[test_tmp_.columns[0]] = test_tmp_[test_tmp_.columns[0]] + test_tmp1[col]\n",
    "    \n",
    "train_T05 = pd.concat([train_T05, train_tmp_], axis=1)\n",
    "test_T05 = pd.concat([test_T05, test_tmp_], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp__ = pd.concat([train_tmp2[train_tmp2.columns[0]]], axis=1)\n",
    "test_tmp__ = pd.concat([test_tmp2[test_tmp2.columns[0]]], axis=1)\n",
    "\n",
    "for col in train_tmp2.columns[1:]:\n",
    "    train_tmp__[train_tmp__.columns[0]] = train_tmp__[train_tmp__.columns[0]] + train_tmp2[col]\n",
    "for col in test_tmp2.columns[1:]:\n",
    "    test_tmp__[test_tmp__.columns[0]] = test_tmp__[test_tmp__.columns[0]] + test_tmp2[col]\n",
    "\n",
    "train_T10 = pd.concat([train_T10, train_tmp__], axis=1)\n",
    "test_T10 = pd.concat([test_T10, test_tmp__], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp___ = pd.concat([train_tmp3[train_tmp3.columns[0]]], axis=1)\n",
    "test_tmp___ = pd.concat([test_tmp3[test_tmp3.columns[0]]], axis=1)\n",
    "\n",
    "for col in train_tmp3.columns[1:]:\n",
    "    train_tmp___[train_tmp___.columns[0]] = train_tmp___[train_tmp___.columns[0]] + train_tmp3[col]\n",
    "for col in test_tmp3.columns[1:]:\n",
    "    test_tmp___[test_tmp___.columns[0]] = test_tmp___[test_tmp___.columns[0]] + test_tmp3[col]\n",
    "\n",
    "train_T01 = pd.concat([train_T01, train_tmp___], axis=1)\n",
    "test_T01 = pd.concat([test_T01, test_tmp___], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Y_Quality 제거, PRODUCT_ID 제거\n",
    "train_quailty1 = train_T05['Y_Quality']\n",
    "train_quailty2 = train_T10['Y_Quality']\n",
    "train_quailty3 = train_T01['Y_Quality']\n",
    "\n",
    "test_name1 = test_T05['PRODUCT_ID']\n",
    "test_name2 = test_T10['PRODUCT_ID']\n",
    "test_name3 = test_T01['PRODUCT_ID']\n",
    "\n",
    "test_T05 = test_T05.drop(['PRODUCT_ID'], axis=1)\n",
    "test_T10 = test_T10.drop(['PRODUCT_ID'], axis=1)\n",
    "test_T01 = test_T01.drop(['PRODUCT_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(train_y1)\n",
    "a = pd.concat([a], ignore_index=True)\n",
    "int_T05 = pd.concat([train_T05, a], axis=1)\n",
    "\n",
    "a = pd.DataFrame(train_y2)\n",
    "a = pd.concat([a], ignore_index=True)\n",
    "int_T10 = pd.concat([train_T10, a], axis=1)\n",
    "\n",
    "a = pd.DataFrame(train_y3)\n",
    "a = pd.concat([a], ignore_index=True)\n",
    "int_T01 = pd.concat([train_T01, a], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val셋 제작\n",
    "val_T05_0 = int_T05[int_T05['Y_Class']==0][:8] # 41\n",
    "val_T05_1 = int_T05[int_T05['Y_Class']==1][:9] # 46\n",
    "val_T05_2 = int_T05[int_T05['Y_Class']==2][:6] # 33\n",
    "val_T05 = pd.concat([val_T05_0, val_T05_1, val_T05_2]) # 120\n",
    "\n",
    "val_T10_0 = int_T10[int_T10['Y_Class']==0][:5] # 28\n",
    "val_T10_1 = int_T10[int_T10['Y_Class']==1][:57] # 289\n",
    "val_T10_2 = int_T10[int_T10['Y_Class']==2][:6] # 32\n",
    "val_T10 = pd.concat([val_T10_0, val_T10_1, val_T10_2]) # 349\n",
    "\n",
    "val_T01_0 = int_T01[int_T01['Y_Class']==0][:4] # 19\n",
    "val_T01_1 = int_T01[int_T01['Y_Class']==1][:14] # 72\n",
    "val_T01_2 = int_T01[int_T01['Y_Class']==2][:7] # 38\n",
    "val_T01 = pd.concat([val_T01_0, val_T01_1, val_T01_2]) # 129\n",
    "\n",
    "# 다시 고치기\n",
    "train_T05_ = pd.concat([int_T05[int_T05['Y_Class']==0][8:], int_T05[int_T05['Y_Class']==1][9:], int_T05[int_T05['Y_Class']==2][6:]], ignore_index=True)\n",
    "train_T10_ = pd.concat([int_T10[int_T10['Y_Class']==0][5:], int_T10[int_T10['Y_Class']==1][57:], int_T10[int_T10['Y_Class']==2][6:]], ignore_index=True)\n",
    "train_T01_ = pd.concat([int_T01[int_T01['Y_Class']==0][4:], int_T01[int_T01['Y_Class']==1][14:], int_T01[int_T01['Y_Class']==2][7:]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y 떼기\n",
    "train_y1_ = train_T05_['Y_Class']\n",
    "train_T05_ = train_T05_.drop(['Y_Class'], axis=1)\n",
    "train_y2_ = train_T10_['Y_Class']\n",
    "train_T10_ = train_T10_.drop(['Y_Class'], axis=1)\n",
    "train_y3_ = train_T01_['Y_Class']\n",
    "train_T01_ = train_T01_.drop(['Y_Class'], axis=1)\n",
    "\n",
    "val_y1 = val_T05['Y_Class']\n",
    "val_T05 = val_T05.drop(['Y_Class'], axis=1)\n",
    "val_y2 = val_T10['Y_Class']\n",
    "val_T10 = val_T10.drop(['Y_Class'], axis=1)\n",
    "val_y3 = val_T01['Y_Class']\n",
    "val_T01 = val_T01.drop(['Y_Class'], axis=1)\n",
    "\n",
    "train_quailty1_ = train_T05_['Y_Quality']\n",
    "train_quailty2_ = train_T10_['Y_Quality']\n",
    "train_quailty3_ = train_T01_['Y_Quality']\n",
    "\n",
    "train_T05_ = train_T05_.drop(['Y_Quality'], axis=1)\n",
    "train_T10_ = train_T10_.drop(['Y_Quality'], axis=1)\n",
    "train_T01_ = train_T01_.drop(['Y_Quality'], axis=1)\n",
    "\n",
    "val_quality1 = val_T05['Y_Quality']\n",
    "val_quality2 = val_T10['Y_Quality']\n",
    "val_quality3 = val_T01['Y_Quality']\n",
    "\n",
    "val_T05 = val_T05.drop(['Y_Quality'], axis=1)\n",
    "val_T10 = val_T10.drop(['Y_Quality'], axis=1)\n",
    "val_T01 = val_T01.drop(['Y_Quality'], axis=1)\n",
    "\n",
    "train_T05 = train_T05.drop(['Y_Quality'], axis=1)\n",
    "train_T10 = train_T10.drop(['Y_Quality'], axis=1)\n",
    "train_T01 = train_T01.drop(['Y_Quality'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9575475923191602/0.8984355274504304/0.9595075502518129\n",
      "0.24136268025594054/-0.40425113941786983/0.23916134768960617\n",
      "Val Regressed.\n"
     ]
    }
   ],
   "source": [
    "# val set 평가\n",
    "RF1 = RandomForestRegressor(n_estimators=300, random_state=cfg.seed).fit(train_T05_, train_quailty1_)\n",
    "RF2 = RandomForestRegressor(n_estimators=100, random_state=cfg.seed).fit(train_T10_, train_quailty2_)\n",
    "RF3 = RandomForestRegressor(n_estimators=150, random_state=cfg.seed).fit(train_T01_, train_quailty3_)\n",
    "\n",
    "val_score1 = RF1.score(val_T05, val_quality1)\n",
    "val_score2 = RF2.score(val_T10, val_quality2)\n",
    "val_score3 = RF3.score(val_T01, val_quality3)\n",
    "\n",
    "print(f'{RF1.score(train_T05_, train_quailty1_)}/{RF2.score(train_T10_, train_quailty2_)}/{RF3.score(train_T01_, train_quailty3_)}')\n",
    "print(f'{val_score1}/{val_score2}/{val_score3}')\n",
    "\n",
    "print('Val Regressed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressed.\n"
     ]
    }
   ],
   "source": [
    "# 전체 train set 학습\n",
    "RF1 = RandomForestRegressor(n_estimators=100, random_state=cfg.seed).fit(train_T05, train_quailty1)\n",
    "RF2 = RandomForestRegressor(n_estimators=100, random_state=cfg.seed).fit(train_T10, train_quailty2)\n",
    "RF3 = RandomForestRegressor(n_estimators=100, random_state=cfg.seed).fit(train_T01, train_quailty3)\n",
    "print('Regressed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# test 평가\n",
    "pred_quality1 = RF1.predict(test_T05)\n",
    "pred_quality2 = RF2.predict(test_T10)\n",
    "pred_quality3 = RF3.predict(test_T01)\n",
    "\n",
    "for item in [pred_quality3, pred_quality2, pred_quality1]:\n",
    "    item = item.reshape(-1,1)\n",
    "    item2 = []\n",
    "    for i in range(len(item)):\n",
    "        item2.append(item[i][0])\n",
    "    item = item2\n",
    "\n",
    "train_quailty1_ = pd.concat([train_quailty1_], ignore_index=True)\n",
    "train_quailty2_ = pd.concat([train_quailty2_], ignore_index=True)\n",
    "train_quailty3_ = pd.concat([train_quailty3_], ignore_index=True)\n",
    "\n",
    "train_T05['Y_Quality'] = train_quailty1\n",
    "train_T10['Y_Quality'] = train_quailty2\n",
    "train_T01['Y_Quality'] = train_quailty3\n",
    "\n",
    "test_T05['Y_Quality'] = pred_quality1\n",
    "test_T10['Y_Quality'] = pred_quality2\n",
    "test_T01['Y_Quality'] = pred_quality3\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Y_Quality 추가\n",
    "train_T05_['Y_Quality'] = train_quailty1_\n",
    "train_T10_['Y_Quality'] = train_quailty2_\n",
    "train_T01_['Y_Quality'] = train_quailty3_\n",
    "\n",
    "val_T05['Y_Quality'] = val_quality1\n",
    "val_T10['Y_Quality'] = val_quality2\n",
    "val_T01['Y_Quality'] = val_quality3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0/1.0/1.0\n",
      "0.391304347826087/0.8382352941176471/0.68\n",
      "Val Classified.\n"
     ]
    }
   ],
   "source": [
    "# train_y1.value_counts() # 41/46/33 {0:1.0, 1:1.0, 2:1.0}\n",
    "# train_y2.value_counts() # 28/289/32 {0:1.5, 1:1.0, 2:1.5}\n",
    "# train_y3.value_counts() # 19/72/38 {0:1.2, 1:1.0, 2:1.2}\n",
    "\n",
    "# classification val set 평가\n",
    "RFC1 = RandomForestClassifier(n_estimators=300, max_depth=5, random_state=cfg.seed, class_weight={0:1.2, 1:1.0, 2:1.2}).fit(train_T05_, train_y1_)\n",
    "RFC2 = RandomForestClassifier(n_estimators=100, random_state=cfg.seed, class_weight={0:1.2, 1:1.0, 2:1.2}).fit(train_T10_, train_y2_)\n",
    "RFC3 = RandomForestClassifier(n_estimators=150,  random_state=cfg.seed, class_weight={0:1.2, 1:1.0, 2:1.2}).fit(train_T01_, train_y3_)\n",
    "\n",
    "val_score1 = RFC1.score(val_T05, val_y1)\n",
    "val_score2 = RFC2.score(val_T10, val_y2)\n",
    "val_score3 = RFC3.score(val_T01, val_y3)\n",
    "\n",
    "print(f'{RFC1.score(train_T05_, train_y1_)}/{RFC2.score(train_T10_, train_y2_)}/{RFC3.score(train_T01_, train_y3_)}')\n",
    "print(f'{val_score1}/{val_score2}/{val_score3}')\n",
    "\n",
    "print('Val Classified.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified.\n"
     ]
    }
   ],
   "source": [
    "# weights = {0:1.2, 1:1.0, 2:1.2}\n",
    "RFC1 = RandomForestClassifier(n_estimators=300, max_depth=5, random_state=cfg.seed, class_weight={0:1.2, 1:1.0, 2:1.2}).fit(train_T05, train_y1)\n",
    "RFC2 = RandomForestClassifier(n_estimators=100, random_state=cfg.seed, class_weight={0:1.2, 1:1.0, 2:1.2}).fit(train_T10, train_y2)\n",
    "RFC3 = RandomForestClassifier(n_estimators=150,  random_state=cfg.seed, class_weight={0:1.2, 1:1.0, 2:1.2}).fit(train_T01, train_y3)\n",
    "\n",
    "print('Classified.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1 = RFC1.predict(test_T05)\n",
    "preds2 = RFC2.predict(test_T10)\n",
    "preds3 = RFC3.predict(test_T01)\n",
    "\n",
    "preds1 = pd.DataFrame(preds1, columns=['Y_Class'])\n",
    "preds2 = pd.DataFrame(preds2, columns=['Y_Class'])\n",
    "preds3 = pd.DataFrame(preds3, columns=['Y_Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.concat([test_name1], ignore_index=True)\n",
    "b = pd.concat([test_name2], ignore_index=True)\n",
    "c = pd.concat([test_name3], ignore_index=True)\n",
    "\n",
    "preds1 = pd.concat([a, preds1], axis=1)\n",
    "preds2 = pd.concat([b, preds2], axis=1)\n",
    "preds3 = pd.concat([c, preds3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.concat([preds1, preds2, preds3])\n",
    "preds = pd.concat([preds], ignore_index=True).sort_values('PRODUCT_ID', ignore_index=True).drop(['PRODUCT_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/264/7\n"
     ]
    }
   ],
   "source": [
    "submit = pd.read_csv('./data/sample_submission.csv')\n",
    "submit['Y_Class'] = preds\n",
    "print(f'{len(submit[submit[\"Y_Class\"]==0])}/{len(submit[submit[\"Y_Class\"]==1])}/{len(submit[submit[\"Y_Class\"]==2])}')\n",
    "submit.to_csv('./baseline_submission_split_최종.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc52f4dc5ab40c4a752199c2d9990189c64af4376a98340362d445a8db0149ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
